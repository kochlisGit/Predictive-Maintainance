{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Advanced Training Algorithms\n",
    "\n",
    "The main problem of this dataset, as also mentioned in the dataset analysis methodology, is the class imbalance.\n",
    "More specifically, 54% samples are labeled as \"*functional*\", 38.5% are labeled as \"*non functional*\" and finally,\n",
    "only 7.5% of the samples belong to \"*functional need repair*\" category. The class imbalance problem can be efficiently\n",
    "dealt with the approaches below:\n",
    "1. Re-Sampling\n",
    "2. Re-Weighting\n",
    "3. Probability Calibration\n",
    "4. Semi-Supervised Learning\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((59400, 21), (59400,), (14850, 21))"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_inputs = pd.read_csv('train_inputs.csv')\n",
    "train_targets = pd.read_csv('targets.csv')\n",
    "test_inputs = pd.read_csv('test_inputs.csv')\n",
    "\n",
    "test_input_ids = test_inputs['id']\n",
    "columns_to_drop = [\n",
    "    'id', 'date_recorded', 'longitude', 'latitude', 'wpt_name', 'num_private', 'subvillage', 'region', 'ward', 'recorded_by',\n",
    "    'scheme_name', 'construction_year', 'extraction_type', 'extraction_type_class', 'management_group', 'payment', 'quality_group',\n",
    "    'quantity_group', 'source_type', 'source_class', 'waterpoint_type_group',\n",
    "]\n",
    "train_inputs = train_inputs.drop(columns=columns_to_drop)\n",
    "test_inputs = test_inputs.drop(columns=columns_to_drop)\n",
    "\n",
    "train_targets = train_targets['status_group'].replace({'functional': 0, 'functional needs repair': 1, 'non functional': 2}).astype(int)\n",
    "train_inputs.shape, train_targets.shape, test_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Synthetic Data & Re-Sampling\n",
    "\n",
    "Re-sampling can be divided into **Over-Sampling the minority samples, or Under-Sampling the majority samples**.\n",
    "The Over-Sampling involves duplicating or generating synthetic samples in the minority class with replacement,\n",
    "while the Under-Sampling involves deleting samples of the majority class. On the other hand,\n",
    "synthetic data involves generating Both approaches can be repeated until\n",
    "the desired class distribution is achieved in the training dataset, such as an equal split across the classes.\n",
    "The algorithms that will be tested are:\n",
    "1. `SVM-Smote` (Over-Sampling)\n",
    "2. `ADASYN` (Over-Sampling)\n",
    "3. `NearMiss v3)` (Under-Sampling)\n",
    "4. `Smote + TOMEK Links` (Combination)\n",
    "5. `NearMiss v3 + SVM-Smote` (Combination)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Resampler: None -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84      3275\n",
      "           1       0.48      0.32      0.38       419\n",
      "           2       0.82      0.78      0.80      2246\n",
      "\n",
      "    accuracy                           0.80      5940\n",
      "   macro avg       0.70      0.66      0.67      5940\n",
      "weighted avg       0.79      0.80      0.79      5940\n",
      "\n",
      "\n",
      "----- Resampler: SVM-Smote -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.82      3275\n",
      "           1       0.40      0.42      0.41       419\n",
      "           2       0.80      0.80      0.80      2246\n",
      "\n",
      "    accuracy                           0.79      5940\n",
      "   macro avg       0.68      0.68      0.68      5940\n",
      "weighted avg       0.79      0.79      0.79      5940\n",
      "\n",
      "\n",
      "----- Resampler: ADASYN -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82      3275\n",
      "           1       0.38      0.46      0.41       419\n",
      "           2       0.80      0.80      0.80      2246\n",
      "\n",
      "    accuracy                           0.78      5940\n",
      "   macro avg       0.67      0.69      0.68      5940\n",
      "weighted avg       0.78      0.78      0.78      5940\n",
      "\n",
      "\n",
      "----- Resampler: NearMiss -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.56      0.66      3275\n",
      "           1       0.17      0.68      0.27       419\n",
      "           2       0.73      0.65      0.69      2246\n",
      "\n",
      "    accuracy                           0.60      5940\n",
      "   macro avg       0.57      0.63      0.54      5940\n",
      "weighted avg       0.74      0.60      0.64      5940\n",
      "\n",
      "\n",
      "----- Resampler: Smote-TOMEK -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82      3275\n",
      "           1       0.36      0.45      0.40       419\n",
      "           2       0.81      0.79      0.80      2246\n",
      "\n",
      "    accuracy                           0.78      5940\n",
      "   macro avg       0.66      0.68      0.67      5940\n",
      "weighted avg       0.79      0.78      0.78      5940\n",
      "\n",
      "\n",
      "----- Resampler: NearMiss + SVM-Smote -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82      3275\n",
      "           1       0.36      0.45      0.40       419\n",
      "           2       0.81      0.79      0.80      2246\n",
      "\n",
      "    accuracy                           0.78      5940\n",
      "   macro avg       0.66      0.68      0.67      5940\n",
      "weighted avg       0.79      0.78      0.78      5940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SVMSMOTE, ADASYN\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    train_inputs, train_targets, test_size=0.1\n",
    ")\n",
    "encoder = OrdinalEncoder()\n",
    "x_train = encoder.fit_transform(x_train)\n",
    "x_test = encoder.transform(x_test)\n",
    "\n",
    "random_state = 0\n",
    "\n",
    "resampling_methods = {\n",
    "    'None': None,\n",
    "    'SVM-Smote': SVMSMOTE(random_state=random_state, n_jobs=-1),\n",
    "    'ADASYN': ADASYN(random_state=random_state, n_jobs=-1),\n",
    "    'NearMiss': NearMiss(version=3, n_jobs=-1),\n",
    "    'Smote-TOMEK': SMOTETomek(random_state=random_state, n_jobs=-1),\n",
    "    'NearMiss + SVM-Smote': [NearMiss(version=3, n_jobs=-1), SMOTETomek(random_state=random_state, n_jobs=-1)]\n",
    "}\n",
    "\n",
    "for resampling_method, resampler in resampling_methods.items():\n",
    "    print(f'\\n----- Resampler: {resampling_method} -----')\n",
    "\n",
    "    if resampling_method == 'None':\n",
    "        x_train_new = x_train\n",
    "        y_train_new = y_train\n",
    "    elif resampling_method != 'NearMiss + SVM-Smote':\n",
    "        x_train_new, y_train_new = resampler.fit_resample(x_train, y_train)\n",
    "    else:\n",
    "        for res in resampler:\n",
    "            x_train_new, y_train_new = res.fit_resample(x_train, y_train)\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=random_state, n_jobs=-1)\n",
    "    clf.fit(x_train_new, y_train_new)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Overall, the **SVM-Smote approach** increases the recall, as well as the `F1` scores of classes `1 & 2` and\n",
    "all approaches increase the `F1` metric for the class `1`, except `NearMiss`.\n",
    "**However, the overall accuracy drops as well**.\n",
    "\n",
    "| Synthetic Data / Re-Sampling | Accuracy | F1 (0)   | F1 (1)   | F1 (2)   |\n",
    "|------------------------------|----------|----------|----------|----------|\n",
    "| None                         | **0.80** | **0.84** | 0.38     | 0.80     |\n",
    "| SVM-Smote                    | 0.79     | 0.82     | **0.41** | **0.80** |\n",
    "| ADASYN                       | 0.78     | 0.82     | 0.41     | 0.80     |\n",
    "| NearMiss                     | 0.60     | 0.66     | 0.27     | 0.60     |\n",
    "| Smote-TOMEK                  | 0.78     | 0.82     | 0.40     | 0.80     |\n",
    "| NearMiss + SVM-Smote         | 0.78     | 0.82     | 0.40     | 0.80     |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Re-Weighting\n",
    "\n",
    "Although re-sampling strategies balance out the dataset, it does not directly tackle the issues caused by\n",
    "Class Imbalance, rather it risks introducing new issues. Since Oversampling introduces duplicate samples,\n",
    "it could easily slow down the training and also lead to over-fitting a model. On the other hand, under-sampling\n",
    "removes certain number of samples. This could lead to the model missing out on learning certain important concepts\n",
    "that it could have learnt from the samples that were removed as a result of.\n",
    "\n",
    "One way to deal with the above issues is to directly modify the loss function. This method involves assigning different\n",
    "weights to different classes (or even different samples). This method is adaptive and there are many variants of this\n",
    "type of method. The simplest is to weight according to the portion of the samples of each category."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Class Weights: None -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84      3275\n",
      "           1       0.48      0.32      0.38       419\n",
      "           2       0.82      0.78      0.80      2246\n",
      "\n",
      "    accuracy                           0.80      5940\n",
      "   macro avg       0.70      0.66      0.67      5940\n",
      "weighted avg       0.79      0.80      0.79      5940\n",
      "\n",
      "\n",
      "----- Class Weights: balanced -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83      3275\n",
      "           1       0.41      0.40      0.40       419\n",
      "           2       0.82      0.79      0.80      2246\n",
      "\n",
      "    accuracy                           0.79      5940\n",
      "   macro avg       0.68      0.68      0.68      5940\n",
      "weighted avg       0.79      0.79      0.79      5940\n",
      "\n",
      "\n",
      "----- Class Weights: balanced_subsample -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83      3275\n",
      "           1       0.40      0.39      0.39       419\n",
      "           2       0.82      0.78      0.80      2246\n",
      "\n",
      "    accuracy                           0.79      5940\n",
      "   macro avg       0.68      0.67      0.68      5940\n",
      "weighted avg       0.79      0.79      0.79      5940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = [None, 'balanced', 'balanced_subsample']\n",
    "\n",
    "for class_weight in class_weights:\n",
    "    print(f'\\n----- Class Weights: {class_weight} -----')\n",
    "\n",
    "    clf = RandomForestClassifier(class_weight=class_weight, random_state=random_state, n_jobs=-1)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The balanced method again increased the `F1` scores of the class: `1`.\n",
    "There was nothing special about this method, however it always worth\n",
    "checking if this method improves the classifier's performance, as it\n",
    "does not add any additional overhead.\n",
    "\n",
    "| Class Weights      | Accuracy | F1 (0)   | F1 (1)   | F1 (2)   |\n",
    "|--------------------|----------|----------|----------|----------|\n",
    "| None               | 0.80     | **0.84** | 0.38     | 0.80     |\n",
    "| Balanced           | **0.80** | 0.83     | **0.40** | **0.80** |\n",
    "| Balanced Subsample | 0.79     | 0.83     | 0.39     | 0.80     |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Probability Calibration\n",
    "\n",
    "Probability calibration is the process of calibrating a model to return the true likelihood of an event.\n",
    "This is necessary when we need the probability of the event in question rather than its classification.\n",
    "The probability can be used as a measure of uncertainty on those problems where a probabilistic prediction\n",
    "is required. This is particularly the case in imbalanced classification, where crisp class labels are often\n",
    "insufficient both in terms of evaluating and selecting a model. The calibration methods that will be used\n",
    "are:\n",
    "1. `Sigmoid`\n",
    "2. `Isotonic`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Calibration Method: None -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84      3275\n",
      "           1       0.48      0.32      0.38       419\n",
      "           2       0.82      0.78      0.80      2246\n",
      "\n",
      "    accuracy                           0.80      5940\n",
      "   macro avg       0.70      0.66      0.67      5940\n",
      "weighted avg       0.79      0.80      0.79      5940\n",
      "\n",
      "\n",
      "----- Calibration Method: sigmoid -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.89      0.85      3275\n",
      "           1       0.54      0.27      0.36       419\n",
      "           2       0.83      0.79      0.81      2246\n",
      "\n",
      "    accuracy                           0.81      5940\n",
      "   macro avg       0.73      0.65      0.67      5940\n",
      "weighted avg       0.80      0.81      0.80      5940\n",
      "\n",
      "\n",
      "----- Calibration Method: isotonic -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.91      0.85      3275\n",
      "           1       0.60      0.25      0.35       419\n",
      "           2       0.85      0.76      0.81      2246\n",
      "\n",
      "    accuracy                           0.81      5940\n",
      "   macro avg       0.75      0.64      0.67      5940\n",
      "weighted avg       0.80      0.81      0.80      5940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "calibration_methods = [None, 'sigmoid', 'isotonic']\n",
    "\n",
    "for calibration_method in calibration_methods:\n",
    "    print(f'\\n----- Calibration Method: {calibration_method} -----')\n",
    "\n",
    "    if calibration_method is not None:\n",
    "        clf = CalibratedClassifierCV(\n",
    "            RandomForestClassifier(random_state=random_state), method=calibration_method,  n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        clf = RandomForestClassifier(random_state=random_state,  n_jobs=-1)\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Calibration | Accuracy | F1 (0)   | F1 (1)   | F1 (2)   |\n",
    "|-------------|----------|----------|----------|----------|\n",
    "| None        | 0.80     | 0.84     | **0.38** | 0.80     |\n",
    "| Sigmoid     | **0.81** | **0.85** | 0.36     | **0.81** |\n",
    "| Isotonic    | 0.81     | 0.85     | 0.35     | 0.81     |\n",
    "\n",
    "The calibrated classifier managed to outperform the original classifier overall."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Semi-Supervised Learning\n",
    "\n",
    "Semi-Supervised Learning is utilized when there are lots of unlabeled data. This happens very often\n",
    "in real world situations, where finding a dataset is quite difficult. Hiring experts to label all the\n",
    "available data might be extremely expensive and time-consuming. In such situations, Semi-Supervised\n",
    "learning comes very handy. In Semi-Supervised Learning, a \"teacher\" classifier is trained on the\n",
    "training set and labels the unseen data. Then, a student classifier is trained using the training set,\n",
    "as well as the most confident labeled samples by the teacher classifier.\n",
    "\n",
    "In this problem, the most confident labeled samples of the classes \"*functional needs repair*\" and \"*non functional*\"\n",
    "can be picked, in order to balance the entire dataset. Additionally, 3 classifiers will be trained as \"teachers\" and\n",
    "the labels will be picked using the \"*voting*\" method. Finally, the teacher classifiers will be calibrated, in order\n",
    "to produce more confident predictions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "       y_true  rf_preds     rf-p0     rf-p1     rf-p2  knn-preds    knn-p0  \\\n14918       0         2  0.122959  0.035956  0.841085          0  0.754766   \n4314        0         0  0.716797  0.043115  0.240088          0  0.631735   \n53553       0         0  0.910562  0.033324  0.056114          0  0.819007   \n6630        2         2  0.106652  0.033385  0.859964          2  0.300715   \n28310       0         0  0.913441  0.032946  0.053614          0  0.819007   \n...       ...       ...       ...       ...       ...        ...       ...   \n16034       0         0  0.871472  0.041846  0.086682          0  0.800681   \n43140       0         0  0.670356  0.269081  0.060563          0  0.557974   \n25161       2         2  0.067407  0.032326  0.900267          2  0.178310   \n9321        0         0  0.909991  0.033340  0.056668          0  0.819007   \n11717       2         0  0.481234  0.069727  0.449039          2  0.236359   \n\n         knn-p1    knn-p2  xgboost-preds  xgboost-p0  xgboost-p1  xgboost-p2  \n14918  0.044403  0.200831              2    0.042181    0.027059    0.930760  \n4314   0.075980  0.292285              0    0.771052    0.027452    0.201496  \n53553  0.044298  0.136694              0    0.877283    0.031460    0.091257  \n6630   0.043180  0.656105              2    0.068640    0.030444    0.900916  \n28310  0.044298  0.136694              0    0.890077    0.029158    0.080765  \n...         ...       ...            ...         ...         ...         ...  \n16034  0.044337  0.154983              0    0.854543    0.069175    0.076282  \n43140  0.123217  0.318809              0    0.906149    0.044213    0.049638  \n25161  0.042853  0.778836              2    0.047351    0.026956    0.925693  \n9321   0.044298  0.136694              0    0.903621    0.028921    0.067459  \n11717  0.110101  0.653540              2    0.105581    0.062296    0.832123  \n\n[5940 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y_true</th>\n      <th>rf_preds</th>\n      <th>rf-p0</th>\n      <th>rf-p1</th>\n      <th>rf-p2</th>\n      <th>knn-preds</th>\n      <th>knn-p0</th>\n      <th>knn-p1</th>\n      <th>knn-p2</th>\n      <th>xgboost-preds</th>\n      <th>xgboost-p0</th>\n      <th>xgboost-p1</th>\n      <th>xgboost-p2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14918</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0.122959</td>\n      <td>0.035956</td>\n      <td>0.841085</td>\n      <td>0</td>\n      <td>0.754766</td>\n      <td>0.044403</td>\n      <td>0.200831</td>\n      <td>2</td>\n      <td>0.042181</td>\n      <td>0.027059</td>\n      <td>0.930760</td>\n    </tr>\n    <tr>\n      <th>4314</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.716797</td>\n      <td>0.043115</td>\n      <td>0.240088</td>\n      <td>0</td>\n      <td>0.631735</td>\n      <td>0.075980</td>\n      <td>0.292285</td>\n      <td>0</td>\n      <td>0.771052</td>\n      <td>0.027452</td>\n      <td>0.201496</td>\n    </tr>\n    <tr>\n      <th>53553</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.910562</td>\n      <td>0.033324</td>\n      <td>0.056114</td>\n      <td>0</td>\n      <td>0.819007</td>\n      <td>0.044298</td>\n      <td>0.136694</td>\n      <td>0</td>\n      <td>0.877283</td>\n      <td>0.031460</td>\n      <td>0.091257</td>\n    </tr>\n    <tr>\n      <th>6630</th>\n      <td>2</td>\n      <td>2</td>\n      <td>0.106652</td>\n      <td>0.033385</td>\n      <td>0.859964</td>\n      <td>2</td>\n      <td>0.300715</td>\n      <td>0.043180</td>\n      <td>0.656105</td>\n      <td>2</td>\n      <td>0.068640</td>\n      <td>0.030444</td>\n      <td>0.900916</td>\n    </tr>\n    <tr>\n      <th>28310</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.913441</td>\n      <td>0.032946</td>\n      <td>0.053614</td>\n      <td>0</td>\n      <td>0.819007</td>\n      <td>0.044298</td>\n      <td>0.136694</td>\n      <td>0</td>\n      <td>0.890077</td>\n      <td>0.029158</td>\n      <td>0.080765</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16034</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.871472</td>\n      <td>0.041846</td>\n      <td>0.086682</td>\n      <td>0</td>\n      <td>0.800681</td>\n      <td>0.044337</td>\n      <td>0.154983</td>\n      <td>0</td>\n      <td>0.854543</td>\n      <td>0.069175</td>\n      <td>0.076282</td>\n    </tr>\n    <tr>\n      <th>43140</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.670356</td>\n      <td>0.269081</td>\n      <td>0.060563</td>\n      <td>0</td>\n      <td>0.557974</td>\n      <td>0.123217</td>\n      <td>0.318809</td>\n      <td>0</td>\n      <td>0.906149</td>\n      <td>0.044213</td>\n      <td>0.049638</td>\n    </tr>\n    <tr>\n      <th>25161</th>\n      <td>2</td>\n      <td>2</td>\n      <td>0.067407</td>\n      <td>0.032326</td>\n      <td>0.900267</td>\n      <td>2</td>\n      <td>0.178310</td>\n      <td>0.042853</td>\n      <td>0.778836</td>\n      <td>2</td>\n      <td>0.047351</td>\n      <td>0.026956</td>\n      <td>0.925693</td>\n    </tr>\n    <tr>\n      <th>9321</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.909991</td>\n      <td>0.033340</td>\n      <td>0.056668</td>\n      <td>0</td>\n      <td>0.819007</td>\n      <td>0.044298</td>\n      <td>0.136694</td>\n      <td>0</td>\n      <td>0.903621</td>\n      <td>0.028921</td>\n      <td>0.067459</td>\n    </tr>\n    <tr>\n      <th>11717</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0.481234</td>\n      <td>0.069727</td>\n      <td>0.449039</td>\n      <td>2</td>\n      <td>0.236359</td>\n      <td>0.110101</td>\n      <td>0.653540</td>\n      <td>2</td>\n      <td>0.105581</td>\n      <td>0.062296</td>\n      <td>0.832123</td>\n    </tr>\n  </tbody>\n</table>\n<p>5940 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "rf_clb = CalibratedClassifierCV(RandomForestClassifier(random_state=random_state, n_jobs=-1))\n",
    "knn_clb = CalibratedClassifierCV(KNeighborsClassifier(weights='distance', n_jobs=-1))\n",
    "xgboost_clb = CalibratedClassifierCV(XGBClassifier(random_state=random_state, n_jobs=-1))\n",
    "\n",
    "rf_clb.fit(x_train, y_train)\n",
    "rf_preds = rf_clb.predict(x_test)\n",
    "rf_probs = rf_clb.predict_proba(x_test)\n",
    "\n",
    "knn_clb.fit(x_train, y_train)\n",
    "knn_preds = knn_clb.predict(x_test)\n",
    "knn_probs = knn_clb.predict_proba(x_test)\n",
    "\n",
    "xgboost_clb.fit(x_train, y_train)\n",
    "xgboost_preds = xgboost_clb.predict(x_test)\n",
    "xgboost_probs = xgboost_clb.predict_proba(x_test)\n",
    "\n",
    "teachers_df = pd.DataFrame({\n",
    "    'y_true': y_test,\n",
    "    'rf_preds': rf_preds, 'rf-p0': rf_probs[:, 0], 'rf-p1': rf_probs[:, 1], 'rf-p2': rf_probs[:, 2],\n",
    "    'knn-preds': knn_preds, 'knn-p0': knn_probs[:, 0], 'knn-p1': knn_probs[:, 1], 'knn-p2': knn_probs[:, 2],\n",
    "    'xgboost-preds': xgboost_preds, 'xgboost-p0': xgboost_probs[:, 0], 'xgboost-p1': xgboost_probs[:, 1], 'xgboost-p2': xgboost_probs[:, 2]\n",
    "})\n",
    "teachers_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "'Selected 3293 labeled samples'"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_threshold = 0.6\n",
    "\n",
    "labeled_samples = []\n",
    "labels = []\n",
    "\n",
    "i = 0\n",
    "for _, row in teachers_df.iterrows():\n",
    "    if row['rf_preds'] == row['knn-preds'] == row['xgboost-preds']:\n",
    "        pred = int(row['rf_preds'])\n",
    "\n",
    "        if row[f'rf-p{pred}'] > confidence_threshold and \\\n",
    "                row[f'knn-p{pred}'] > confidence_threshold and \\\n",
    "                row[f'xgboost-p{pred}'] > confidence_threshold:\n",
    "            labeled_samples.append(x_test.iloc[i])\n",
    "            labels.append(pred)\n",
    "    i += 1\n",
    "\n",
    "f'Selected {len(labeled_samples)} labeled samples'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "((56753, 21), (56753,))"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train_new = np.concatenate([x_train, labeled_samples], axis=0)\n",
    "y_train_new = np.concatenate([y_train, labels], axis=0)\n",
    "\n",
    "x_train_new.shape, y_train_new.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84      3275\n",
      "           1       0.49      0.33      0.39       419\n",
      "           2       0.82      0.79      0.80      2246\n",
      "\n",
      "    accuracy                           0.80      5940\n",
      "   macro avg       0.71      0.66      0.68      5940\n",
      "weighted avg       0.79      0.80      0.79      5940\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=random_state, n_jobs=-1)\n",
    "rf.fit(x_train_new, y_train_new)\n",
    "y_pred = rf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}